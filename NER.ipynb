{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual NER\n",
    "### Load Word Embeddings in Training Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format('data/wiki.multi.en.vec.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (200000, 300)\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = wv.vectors\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "\n",
    "def word2idx(word):\n",
    "  return wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "  return wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Training Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "\n",
    "corpus = ConllCorpusReader('data', fileids=['eng.train.txt'], columntypes=('words', 'pos', 'ne', 'chunk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 11376\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nums_regex = re.compile(r'0+')\n",
    "def clean_sents(iob_sents):\n",
    "    sents = []\n",
    "    # remove sentences shorter than 5 words\n",
    "    for sent in iob_sents:\n",
    "        if len(sent) > 4:\n",
    "            new_sent = []\n",
    "            # clean the words\n",
    "            for word in sent:\n",
    "                this_word = word[0].lower()\n",
    "                new_word = ''\n",
    "                # replace numbers with 0\n",
    "                for char in this_word:\n",
    "                    if char.isalpha():\n",
    "                        new_word = new_word + char\n",
    "                    elif char.isdigit():\n",
    "                        new_word = new_word + '0'\n",
    "                new_word = nums_regex.sub('0', new_word)\n",
    "                new_sent.append((new_word, word[1], word[2]))\n",
    "            sents.append(new_sent)\n",
    "    return sents\n",
    "\n",
    "sents = clean_sents(corpus.iob_sents())\n",
    "print(\"Number of sentences:\", len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while start < len(data):\n",
    "        while end < len(data) and end-start < window_size:\n",
    "            end += 1\n",
    "        yield start, end\n",
    "        start = end\n",
    "        if end >= len(data):\n",
    "            break  \n",
    "        \n",
    "def get_features(sentences, num_features, window_size, wv):\n",
    "    features = np.empty((0, window_size, num_features))\n",
    "    labels = np.empty((0))\n",
    "    count_unk = 0\n",
    "    count_known = 0\n",
    "    for i in range(len(sentences)):\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*i, (i/len(sentences)*100)))\n",
    "        sys.stdout.flush()\n",
    "#         if i % 100 == 0:\n",
    "#             print(\"Processed\", i, \"of\", len(sentences))\n",
    "        sent_features, sent_labels = get_sentence_features(sentences[i], num_features, window_size, wv)\n",
    "        features = np.vstack([features, sent_features])\n",
    "        labels = np.append(labels, sent_labels)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def get_sentence_features(sentence, num_features, window_size, wv):\n",
    "    features = np.empty((0, window_size, num_features))\n",
    "    labels = np.empty((0))\n",
    "    for j in range(len(sentence)):\n",
    "        m = floor(window_size/2)\n",
    "        start = j-m\n",
    "        end = j+m+1\n",
    "        # no padding\n",
    "        if start >= 0 and end <= len(sentence):\n",
    "            words = sentence[start:end]\n",
    "        else:\n",
    "            # padding\n",
    "            if start >= 0:\n",
    "                words = sentence[start:] + [('', '', 'O')] * (end - len(sentence))\n",
    "            elif end <= len(sentence):\n",
    "                words = [('', '', 'O')] * (0-start) + sentence[:end]\n",
    "            else:\n",
    "                [('', '', 'O')] * (0-start) + sentence + [('', '', 'O')] * (end - len(sentence))\n",
    "        emb = []\n",
    "        # clean the words and get the vectors\n",
    "        for word in words:\n",
    "            this_word = word[0]\n",
    "            if this_word in wv.vocab:\n",
    "                emb.append(wv.get_vector(this_word))\n",
    "            elif this_word == '':\n",
    "                emb.append(np.zeros(num_features))\n",
    "            else:\n",
    "                emb.append(np.random.uniform(-0.25,0.25, num_features))  # random vector for unknown\n",
    "        feature_stack = np.dstack([[emb]])\n",
    "        features = np.vstack([features, feature_stack])\n",
    "        labels = np.append(labels, sentence[j][-1])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 11376\n",
      "Processed 100 of 11376\n",
      "Processed 200 of 11376\n",
      "Processed 300 of 11376\n",
      "Processed 400 of 11376\n",
      "Processed 500 of 11376\n",
      "Processed 600 of 11376\n",
      "Processed 700 of 11376\n",
      "Processed 800 of 11376\n",
      "Processed 900 of 11376\n",
      "Processed 1000 of 11376\n",
      "Processed 1100 of 11376\n",
      "Processed 1200 of 11376\n",
      "Processed 1300 of 11376\n",
      "Processed 1400 of 11376\n",
      "Processed 1500 of 11376\n",
      "Processed 1600 of 11376\n",
      "Processed 1700 of 11376\n",
      "Processed 1800 of 11376\n",
      "Processed 1900 of 11376\n",
      "Processed 2000 of 11376\n",
      "Processed 2100 of 11376\n",
      "Processed 2200 of 11376\n",
      "Processed 2300 of 11376\n",
      "Processed 2400 of 11376\n",
      "Processed 2500 of 11376\n",
      "Processed 2600 of 11376\n",
      "Processed 2700 of 11376\n",
      "Processed 2800 of 11376\n",
      "Processed 2900 of 11376\n",
      "Processed 3000 of 11376\n",
      "Processed 3100 of 11376\n",
      "Processed 3200 of 11376\n",
      "Processed 3300 of 11376\n",
      "Processed 3400 of 11376\n",
      "Processed 3500 of 11376\n",
      "Processed 3600 of 11376\n",
      "Processed 3700 of 11376\n",
      "Processed 3800 of 11376\n",
      "Processed 3900 of 11376\n",
      "Processed 4000 of 11376\n",
      "Processed 4100 of 11376\n",
      "Processed 4200 of 11376\n",
      "Processed 4300 of 11376\n",
      "Processed 4400 of 11376\n",
      "Processed 4500 of 11376\n",
      "Processed 4600 of 11376\n",
      "Processed 4700 of 11376\n",
      "Processed 4800 of 11376\n",
      "Processed 4900 of 11376\n",
      "Processed 5000 of 11376\n",
      "Processed 5100 of 11376\n",
      "Processed 5200 of 11376\n",
      "Processed 5300 of 11376\n",
      "Processed 5400 of 11376\n",
      "Processed 5500 of 11376\n",
      "Processed 5600 of 11376\n",
      "Processed 5700 of 11376\n",
      "Processed 5800 of 11376\n",
      "Processed 5900 of 11376\n",
      "Processed 6000 of 11376\n",
      "Processed 6100 of 11376\n",
      "Processed 6200 of 11376\n",
      "Processed 6300 of 11376\n",
      "Processed 6400 of 11376\n",
      "Processed 6500 of 11376\n",
      "Processed 6600 of 11376\n",
      "Processed 6700 of 11376\n",
      "Processed 6800 of 11376\n",
      "Processed 6900 of 11376\n",
      "Processed 7000 of 11376\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "window_size = 7\n",
    "num_features = embedding_size\n",
    "X, y = get_features(sents, num_features, window_size, wv)\n",
    "# y = np.asarray(pd.get_dummies(label_values), dtype = np.float32)\n",
    "# X = X.reshape((len(X), window_size, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "np.save('data/preprocessed.eng.X.train', X)\n",
    "np.save('data/preprocessed.eng.y.train', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Bidirectional, Flatten, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "dropout = 0.05\n",
    "num_classes = len(set(y))\n",
    "print(\"Classes:\", set(y))\n",
    "\n",
    "print(\"num_classes:\",num_classes)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=num_features), input_shape=(window_size, num_features,), merge_mode='concat'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_features, activation='tanh'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "# model.fit(X, y)\n",
    "# print(\"Score:\", model.score(X, y))\n",
    "# kfold = KFold(n_splits=5, shuffle=True)  # StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# results = cross_val_score(model, X, y, cv=kfold)\n",
    "# print(\"Score:\", results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Multilingual Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Target Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_de = KeyedVectors.load_word2vec_format('data/wiki.multi.en.vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_de = ConllCorpusReader('data', fileids=['deu.testa.txt'], columntypes=('words', 'srl', 'pos', 'ne', 'chunk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Target Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sents_de = clean_sents(corpus_de.iob_sents())\n",
    "print(sents_de[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_de, y_de = get_features(sents_de, num_features, window_size, wv_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"English classes:\", set(y))\n",
    "print(\"German classes:\", set(y_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_de = model.score(X_de, y_de)\n",
    "print(\"German Score from English Training:\", score_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_de_10 = model.predict(X_de[:10])\n",
    "print(\"Prediction:\", pred_de_10)\n",
    "print(\"True:\", y_de[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)  # english only results\n",
    "model.fit(X_train, y_train, epochs=1, shuffle=True)\n",
    "y_pred = model.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=list(set(y)))\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(set(y)), normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_de = model.predict(X_de)\n",
    "cnf_matrix = confusion_matrix(y_de, y_pred_de, labels=list(set(y_de)))\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(set(y_de)), normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
