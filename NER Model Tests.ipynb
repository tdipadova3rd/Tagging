{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "import nltk\n",
    "import re\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Bidirectional, Flatten, Dropout, TimeDistributed\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 70\n",
    "num_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sentence_features(sentences, num_features, max_length, wv):\n",
    "    features = np.empty((0, max_length, num_features))\n",
    "    for i in range(len(sentences)):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processed\", i, \"of\", len(sentences))\n",
    "        sent = sentences[i]\n",
    "        new_sent = []\n",
    "        for j in range(max_length):\n",
    "            if 0 <= j < len(sent):\n",
    "                this_word = sent[j]\n",
    "                if this_word in wv.vocab:\n",
    "                    new_sent.append(wv.get_vector(this_word))\n",
    "                elif this_word == '':\n",
    "                    new_sent.append(np.zeros(num_features))\n",
    "                else:\n",
    "                    new_sent.append(np.random.uniform(-0.25,0.25, num_features))  # random vector for unknown\n",
    "            else:\n",
    "                new_sent.append(np.zeros(num_features))\n",
    "\n",
    "        feature_stack = np.dstack([[new_sent]])\n",
    "        features = np.vstack([features, feature_stack])\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_regex = re.compile(r'0+')\n",
    "def clean_sents(sents, max_length):\n",
    "    cleaned = []\n",
    "    # remove sentences shorter than 5 words\n",
    "    for sent in sents:\n",
    "        if len(sent) > 4 and len(sent) <= max_length:\n",
    "            new_sent = []\n",
    "            # clean the words\n",
    "            for word in sent:\n",
    "                this_word = word.lower()\n",
    "                new_word = ''\n",
    "                # replace numbers with 0\n",
    "                for char in this_word:\n",
    "                    if char.isalpha():\n",
    "                        new_word = new_word + char\n",
    "                    elif char.isdigit():\n",
    "                        new_word = new_word + '0'\n",
    "                new_word = nums_regex.sub('0', new_word)\n",
    "                new_sent.append((new_word, word[1], word[2]))\n",
    "            cleaned.append(new_sent)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr2label(cats, labels):\n",
    "    new_labels = []\n",
    "    for i in range(len(cats)):\n",
    "        sent_labels = []\n",
    "        for j in range(len(cats[i])):\n",
    "            label = np.argmax(cats[i][j])\n",
    "            label = labels[label]\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format('data/wiki.multi.en.vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_it = KeyedVectors.load_word2vec_format('data/wiki.multi.it.vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['B-MISC', 'I-MISC', 'I-LOC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'B-ORG', 'O']\n",
    "num_classes = len(classes)\n",
    "encoded_classes = range(num_classes)\n",
    "class2idx = {classes[enc]: enc for enc in encoded_classes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "recurrent_dropout = 0.3\n",
    "hidden_nodes = 100\n",
    "window_size = 70\n",
    "\n",
    "def create_model(num_classes, num_features, hidden_nodes=100):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(\n",
    "        LSTM(units=num_features, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout),\n",
    "        input_shape=(window_size, num_features,),\n",
    "        merge_mode='concat'))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(Dense(hidden_nodes, activation='relu')))\n",
    "    # add a CRF layer to enforce NER IOB rules\n",
    "    crf = CRF(num_classes, sparse_target=False)\n",
    "    model.add(crf)\n",
    "    print(\"Summary:\", model.summary())\n",
    "    model.compile(optimizer='rmsprop', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/full.X.train.npy', mmap_mode='r')\n",
    "y_train = np.load('data/full.y.train.npy', mmap_mode='r')\n",
    "model = create_model(num_classes, num_features)\n",
    "model.fit(X_train, y_train, batch_size=50, epochs=10)\n",
    "# save_load_utils.load_all_weights(model,'models/full_train.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent1 = 'The United States of America, often called USA, is the country where we live.' # 'La multinacional española Telefónica ha impuesto un récord mundial al poner en servicio tres millones de nuevas líneas en el estado brasileño de Sao Paulo desde que asumió el control de la operadora Telesp hace 20 meses, anunció hoy el presidente de Telefónica do Brasil, Fernando Xavier Ferreira.'\n",
    "test_sent2 = 'Sai la semplicità di Messer Nicia, che benché sia dottore, egli è el più semplice e il più sciocco uomo di Firenze'\n",
    "test_sent3 = 'You know the simplicity of Messer Nicia, that although he is a doctor, he is the simplest and the silliest man in Florence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 1\n"
     ]
    }
   ],
   "source": [
    "test1 = nltk.word_tokenize(test_sent1)  # tokenize the sentence\n",
    "test1 = clean_sents([test1], max_length)\n",
    "test_X1 = get_padded_sentence_features(test1, num_features, max_length, wv)  # process the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "pred1 = model.predict(test_X1)  # predict\n",
    "labels1 = arr2label(pred1, classes)  # get labels\n",
    "labels1 = labels1[:len(test1)]  # remove padding\n",
    "print(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [(test1[i], labels1[i]) for i in range(len(test1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'O'), ('United', 'O'), ('States', 'O'), ('of', 'O'), ('America', 'O'), (',', 'O'), ('often', 'O'), ('called', 'O'), ('USA', 'O'), (',', 'O'), ('is', 'O'), ('the', 'O'), ('country', 'O'), ('where', 'O'), ('we', 'O'), ('live', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
