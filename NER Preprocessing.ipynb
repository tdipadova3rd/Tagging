{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "import re\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Bidirectional, Flatten, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_regex = re.compile(r'0+')\n",
    "def clean_sents(iob_sents):\n",
    "    sents = []\n",
    "    # remove sentences shorter than 5 words\n",
    "    for sent in iob_sents:\n",
    "        if len(sent) > 4:\n",
    "            new_sent = []\n",
    "            # clean the words\n",
    "            for word in sent:\n",
    "                this_word = word[0].lower()\n",
    "                new_word = ''\n",
    "                # replace numbers with 0\n",
    "                for char in this_word:\n",
    "                    if char.isalpha():\n",
    "                        new_word = new_word + char\n",
    "                    elif char.isdigit():\n",
    "                        new_word = new_word + '0'\n",
    "                new_word = nums_regex.sub('0', new_word)\n",
    "                new_sent.append((new_word, word[1], word[2]))\n",
    "            sents.append(new_sent)\n",
    "    return sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while start < len(data):\n",
    "        while end < len(data) and end-start < window_size:\n",
    "            end += 1\n",
    "        yield start, end\n",
    "        start = end\n",
    "        if end >= len(data):\n",
    "            break  \n",
    "\n",
    "def get_padded_sentence_features(sentences, num_features, max_length, wv):\n",
    "    features = np.empty((0, max_length, num_features))\n",
    "    labels = np.empty((0, max_length))\n",
    "    for i in range(len(sentences)):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processed\", i, \"of\", len(sentences))\n",
    "        sent = sentences[i]\n",
    "        new_sent = []\n",
    "        sent_labels = np.empty((0))\n",
    "        for j in range(max_length):\n",
    "            if 0 <= j < len(sent):\n",
    "                this_word = sent[j][0]\n",
    "                if this_word in wv.vocab:\n",
    "                    new_sent.append(wv.get_vector(this_word))\n",
    "                elif this_word == '':\n",
    "                    new_sent.append(np.zeros(num_features))\n",
    "                else:\n",
    "                    new_sent.append(np.random.uniform(-0.25,0.25, num_features))  # random vector for unknown\n",
    "                sent_labels = np.append(sent_labels, sent[j][-1])\n",
    "            else:\n",
    "                new_sent.append(np.zeros(num_features))\n",
    "                sent_labels = np.append(sent_labels, 'O')\n",
    "\n",
    "        labels = np.vstack([labels, sent_labels])\n",
    "        feature_stack = np.dstack([[new_sent]])\n",
    "        features = np.vstack([features, feature_stack])\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def get_features(sentences, num_features, window_size, wv):\n",
    "    features = np.empty((0, window_size, num_features))\n",
    "    labels = np.empty((0))\n",
    "    count_unk = 0\n",
    "    count_known = 0\n",
    "    for i in range(len(sentences)):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processed\", i, \"of\", len(sentences))\n",
    "        sent_features, sent_labels = get_sentence_features(sentences[i], num_features, window_size, wv)\n",
    "        features = np.vstack([features, sent_features])\n",
    "        labels = np.append(labels, sent_labels)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def get_sentence_features(sentence, num_features, window_size, wv):\n",
    "    features = np.empty((0, window_size, num_features))\n",
    "    labels = np.empty((0))\n",
    "    for j in range(len(sentence)):\n",
    "        m = floor(window_size/2)\n",
    "        start = j-m\n",
    "        end = j+m+1\n",
    "        # no padding\n",
    "        if start >= 0 and end <= len(sentence):\n",
    "            words = sentence[start:end]\n",
    "        else:\n",
    "            # padding\n",
    "            if start >= 0:\n",
    "                words = sentence[start:] + [('', '', 'O')] * (end - len(sentence))\n",
    "            elif end <= len(sentence):\n",
    "                words = [('', '', 'O')] * (0-start) + sentence[:end]\n",
    "            else:\n",
    "                [('', '', 'O')] * (0-start) + sentence + [('', '', 'O')] * (end - len(sentence))\n",
    "        emb = []\n",
    "        # clean the words and get the vectors\n",
    "        for word in words:\n",
    "            this_word = word[0]\n",
    "            if this_word in wv.vocab:\n",
    "                emb.append(wv.get_vector(this_word))\n",
    "            elif this_word == '':\n",
    "                emb.append(np.zeros(num_features))\n",
    "            else:\n",
    "                emb.append(np.random.uniform(-0.25,0.25, num_features))  # random vector for unknown\n",
    "        feature_stack = np.dstack([[emb]])\n",
    "        features = np.vstack([features, feature_stack])\n",
    "        labels = np.append(labels, sentence[j][-1])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Embeddings in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format('data/wiki.multi.en.vec.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (200000, 300)\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = wv.vectors\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "\n",
    "max_length = 70\n",
    "window_size = 7\n",
    "num_features = embedding_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the English Training Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = ConllCorpusReader('data', fileids=['eng.train.txt'], columntypes=('words', 'pos', 'ne', 'chunk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = clean_sents(corpus.iob_sents())\n",
    "print(\"Number of sentences:\", len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 11376\n",
      "Processed 100 of 11376\n",
      "Processed 200 of 11376\n",
      "Processed 300 of 11376\n",
      "Processed 400 of 11376\n",
      "Processed 500 of 11376\n",
      "Processed 600 of 11376\n",
      "Processed 700 of 11376\n",
      "Processed 800 of 11376\n",
      "Processed 900 of 11376\n",
      "Processed 1000 of 11376\n",
      "Processed 1100 of 11376\n",
      "Processed 1200 of 11376\n",
      "Processed 1300 of 11376\n",
      "Processed 1400 of 11376\n",
      "Processed 1500 of 11376\n",
      "Processed 1600 of 11376\n",
      "Processed 1700 of 11376\n",
      "Processed 1800 of 11376\n",
      "Processed 1900 of 11376\n",
      "Processed 2000 of 11376\n",
      "Processed 2100 of 11376\n",
      "Processed 2200 of 11376\n",
      "Processed 2300 of 11376\n",
      "Processed 2400 of 11376\n",
      "Processed 2500 of 11376\n",
      "Processed 2600 of 11376\n",
      "Processed 2700 of 11376\n",
      "Processed 2800 of 11376\n",
      "Processed 2900 of 11376\n",
      "Processed 3000 of 11376\n",
      "Processed 3100 of 11376\n",
      "Processed 3200 of 11376\n",
      "Processed 3300 of 11376\n",
      "Processed 3400 of 11376\n",
      "Processed 3500 of 11376\n",
      "Processed 3600 of 11376\n",
      "Processed 3700 of 11376\n",
      "Processed 3800 of 11376\n",
      "Processed 3900 of 11376\n",
      "Processed 4000 of 11376\n",
      "Processed 4100 of 11376\n",
      "Processed 4200 of 11376\n",
      "Processed 4300 of 11376\n",
      "Processed 4400 of 11376\n",
      "Processed 4500 of 11376\n",
      "Processed 4600 of 11376\n",
      "Processed 4700 of 11376\n",
      "Processed 4800 of 11376\n",
      "Processed 4900 of 11376\n",
      "Processed 5000 of 11376\n",
      "Processed 5100 of 11376\n",
      "Processed 5200 of 11376\n",
      "Processed 5300 of 11376\n",
      "Processed 5400 of 11376\n",
      "Processed 5500 of 11376\n",
      "Processed 5600 of 11376\n",
      "Processed 5700 of 11376\n",
      "Processed 5800 of 11376\n",
      "Processed 5900 of 11376\n",
      "Processed 6000 of 11376\n",
      "Processed 6100 of 11376\n",
      "Processed 6200 of 11376\n",
      "Processed 6300 of 11376\n",
      "Processed 6400 of 11376\n",
      "Processed 6500 of 11376\n",
      "Processed 6600 of 11376\n",
      "Processed 6700 of 11376\n",
      "Processed 6800 of 11376\n",
      "Processed 6900 of 11376\n",
      "Processed 7000 of 11376\n",
      "Processed 7100 of 11376\n",
      "Processed 7200 of 11376\n",
      "Processed 7300 of 11376\n",
      "Processed 7400 of 11376\n",
      "Processed 7500 of 11376\n",
      "Processed 7600 of 11376\n",
      "Processed 7700 of 11376\n",
      "Processed 7800 of 11376\n",
      "Processed 7900 of 11376\n",
      "Processed 8000 of 11376\n",
      "Processed 8100 of 11376\n",
      "Processed 8200 of 11376\n",
      "Processed 8300 of 11376\n",
      "Processed 8400 of 11376\n",
      "Processed 8500 of 11376\n",
      "Processed 8600 of 11376\n",
      "Processed 8700 of 11376\n",
      "Processed 8800 of 11376\n",
      "Processed 8900 of 11376\n",
      "Processed 9000 of 11376\n",
      "Processed 9100 of 11376\n",
      "Processed 9200 of 11376\n",
      "Processed 9300 of 11376\n",
      "Processed 9400 of 11376\n",
      "Processed 9500 of 11376\n",
      "Processed 9600 of 11376\n",
      "Processed 9700 of 11376\n",
      "Processed 9800 of 11376\n",
      "Processed 9900 of 11376\n",
      "Processed 10000 of 11376\n",
      "Processed 10100 of 11376\n",
      "Processed 10200 of 11376\n",
      "Processed 10300 of 11376\n",
      "Processed 10400 of 11376\n",
      "Processed 10500 of 11376\n",
      "Processed 10600 of 11376\n",
      "Processed 10700 of 11376\n",
      "Processed 10800 of 11376\n",
      "Processed 10900 of 11376\n",
      "Processed 11000 of 11376\n",
      "Processed 11100 of 11376\n",
      "Processed 11200 of 11376\n",
      "Processed 11300 of 11376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# X, y = get_features(sents, num_features, window_size, wv)\n",
    "X, y = get_padded_sentence_features(sents, num_features, max_length, wv)\n",
    "\n",
    "np.save('data/eng.X.train.npy', X)\n",
    "np.save('data/eng.y.train.npy', y)\n",
    "# y = np.asarray(pd.get_dummies(label_values), dtype = np.float32)\n",
    "# X = X.reshape((len(X), window_size, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load German Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_de = KeyedVectors.load_word2vec_format('data/wiki.multi.en.vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_de_test = ConllCorpusReader('data', fileids=['deu.testa.txt'], columntypes=('words', 'srl', 'pos', 'ne', 'chunk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('groer', 'NN', 'O'), ('fotowettbeweb', 'NN', 'O'), ('', '$(', 'O'), ('nordendler', 'NN', 'I-ORG'), ('', '$(', 'O'), ('laden', 'VVFIN', 'O'), ('die', 'ART', 'O'), ('nordendler', 'NN', 'I-MISC'), ('ein', 'ART', 'O')], [('einen', 'ART', 'O'), ('fotowettbewerb', 'NN', 'O'), ('mit', 'APPR', 'O'), ('dem', 'ART', 'O'), ('thema', 'NN', 'O'), ('', '$(', 'O'), ('leben', 'NN', 'O'), ('und', 'KON', 'O'), ('feiern', 'VVFIN', 'O'), ('im', 'APPRART', 'O'), ('nordend', 'NN', 'I-LOC'), ('und', 'KON', 'O'), ('mit', 'APPR', 'O'), ('den', 'ART', 'O'), ('nordendlern', 'NN', 'I-LOC'), ('', '$(', 'O'), ('hat', 'VAFIN', 'O'), ('der', 'ART', 'O'), ('vorstand', 'NN', 'O'), ('des', 'ART', 'O'), ('karnevalclubs', 'NN', 'O'), ('ausgeschrieben', 'VVPP', 'O'), ('', '$.', 'O')], [('teilnehmen', 'NN', 'O'), ('knnen', 'VMFIN', 'O'), ('alle', 'PIDAT', 'O'), ('mitglieder', 'NN', 'O'), ('und', 'KON', 'O'), ('freunde', 'NN', 'O'), ('der', 'ART', 'O'), ('', '$(', 'O'), ('nordendler', 'NN', 'I-ORG'), ('', '$(', 'O'), ('sowie', 'KON', 'O'), ('alle', 'PIDAT', 'O'), ('brger', 'NN', 'O'), ('des', 'ART', 'O'), ('stadtteils', 'NN', 'O'), ('', '$.', 'O')], [('fr', 'APPR', 'O'), ('die', 'ART', 'O'), ('schnsten', 'ADJA', 'O'), ('und', 'KON', 'O'), ('originellsten', 'ADJA', 'O'), ('motive', 'NN', 'O'), ('jeder', 'PIDAT', 'O'), ('preisgruppe', 'NN', 'O'), ('gibt', 'VVFIN', 'O'), ('es', 'PPER', 'O'), ('einen', 'ART', 'O'), ('gutschein', 'NN', 'O'), ('ber', 'APPR', 'O'), ('jeweils', 'ADV', 'O'), ('0', 'CARD', 'O'), ('mark', 'NN', 'O'), ('zu', 'PTKZU', 'O'), ('gewinnen', 'VVINF', 'O'), ('', '$.', 'O')], [('ausgeschrieben', 'VVPP', 'O'), ('ist', 'VAFIN', 'O'), ('der', 'ART', 'O'), ('wettbewerb', 'NN', 'O'), ('in', 'APPR', 'O'), ('drei', 'CARD', 'O'), ('preisgruppen', 'NN', 'O'), ('', '$.', 'O')], [('kinder', 'NN', 'O'), ('bis', 'APPR', 'O'), ('0', 'CARD', 'O'), ('jahre', 'NN', 'O'), ('', '$,', 'O'), ('jugendliche', 'NN', 'O'), ('und', 'KON', 'O'), ('erwachsene', 'NN', 'O'), ('bis', 'APPR', 'O'), ('0', 'CARD', 'O'), ('jahre', 'NN', 'O'), ('sowie', 'KON', 'O'), ('erwachsene', 'NN', 'O'), ('ber', 'APPR', 'O'), ('0', 'CARD', 'O'), ('jahre', 'NN', 'O'), ('', '$.', 'O')], [('die', 'ART', 'O'), ('gewinner', 'NN', 'O'), ('werden', 'VAFIN', 'O'), ('beim', 'APPRART', 'O'), ('saisonauftakt', 'NN', 'O'), ('am', 'APPRART', 'O'), ('0', 'ADJA', 'O'), ('november', 'NN', 'O'), ('', '$(', 'O'), ('0', 'CARD', 'O'), ('bis', 'APPR', 'O'), ('0', 'CARD', 'O'), ('uhr', 'NN', 'O'), ('', '$(', 'O'), ('', '$,', 'O'), ('im', 'APPRART', 'O'), ('saal', 'NN', 'O'), ('des', 'ART', 'O'), ('gehrlosenzentrums', 'NN', 'I-LOC'), ('', '$(', 'O'), ('rothschildallee', 'NN', 'I-LOC'), ('0', 'CARD', 'O'), ('a', 'NN', 'O'), ('', '$(', 'O'), ('bekanntgegeben', 'VVPP', 'O'), ('', '$.', 'O')], [('einsendeschlu', 'NN', 'O'), ('der', 'ART', 'O'), ('fotos', 'NN', 'O'), ('mit', 'APPR', 'O'), ('altersangabe', 'NN', 'O'), ('', '$,', 'O'), ('name', 'NN', 'O'), ('und', 'KON', 'O'), ('anschrift', 'NN', 'O'), ('ist', 'VAFIN', 'O'), ('am', 'APPRART', 'O'), ('samstag', 'NN', 'O'), ('', '$,', 'O'), ('0', 'ADJA', 'O'), ('oktober', 'NN', 'O'), ('', '$.', 'O')], [('aufnahmen', 'NN', 'O'), ('', '$(', 'O'), ('bunt', 'ADJD', 'O'), ('oder', 'KON', 'O'), ('schwarzwei', 'ADJD', 'O'), ('', '$(', 'O'), ('knnen', 'VMFIN', 'O'), ('an', 'APPR', 'O'), ('die', 'ART', 'O'), ('geschftsstelle', 'NN', 'O'), ('der', 'ART', 'O'), ('', '$(', 'O'), ('nordendler', 'NN', 'I-ORG'), ('', '$(', 'O'), ('', '$,', 'O'), ('lenaustrae', 'NN', 'I-LOC'), ('0', 'CARD', 'O'), ('', '$(', 'O'), ('glauburgbunker', 'NN', 'I-LOC'), ('', '$(', 'O'), ('', '$,', 'O'), ('geschickt', 'VVPP', 'O'), ('werden', 'VAFIN', 'O'), ('', '$.', 'O')], [('am', 'APPRART', 'O'), ('samstag', 'NN', 'O'), ('', '$,', 'O'), ('0', 'ADJA', 'O'), ('september', 'NN', 'O'), ('', '$(', 'O'), ('0', 'CARD', 'O'), ('bis', 'APPR', 'O'), ('0', 'CARD', 'O'), ('uhr', 'NN', 'O'), ('', '$(', 'O'), ('', '$,', 'O'), ('beteiligen', 'VVFIN', 'O'), ('sich', 'PRF', 'O'), ('die', 'ART', 'O'), ('', '$(', 'O'), ('nordendler', 'NN', 'I-ORG'), ('', '$(', 'O'), ('am', 'APPRART', 'O'), ('oederwegstraenfest', 'NN', 'I-MISC'), ('mit', 'APPR', 'O'), ('einem', 'ART', 'O'), ('stand', 'NN', 'O'), ('', '$(', 'O'), ('frhschoppen', 'NN', 'O'), ('von', 'APPR', 'O'), ('0', 'CARD', 'O'), ('bis', 'APPR', 'O'), ('0', 'CARD', 'O'), ('uhr', 'NN', 'O'), ('', '$(', 'O'), ('', '$.', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "sents_de_test = clean_sents(corpus_de_test.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 2644\n",
      "Processed 100 of 2644\n",
      "Processed 200 of 2644\n",
      "Processed 300 of 2644\n",
      "Processed 400 of 2644\n",
      "Processed 500 of 2644\n",
      "Processed 600 of 2644\n",
      "Processed 700 of 2644\n",
      "Processed 800 of 2644\n",
      "Processed 900 of 2644\n",
      "Processed 1000 of 2644\n",
      "Processed 1100 of 2644\n",
      "Processed 1200 of 2644\n",
      "Processed 1300 of 2644\n",
      "Processed 1400 of 2644\n",
      "Processed 1500 of 2644\n",
      "Processed 1600 of 2644\n",
      "Processed 1700 of 2644\n",
      "Processed 1800 of 2644\n",
      "Processed 1900 of 2644\n",
      "Processed 2000 of 2644\n",
      "Processed 2100 of 2644\n",
      "Processed 2200 of 2644\n",
      "Processed 2300 of 2644\n",
      "Processed 2400 of 2644\n",
      "Processed 2500 of 2644\n",
      "Processed 2600 of 2644\n"
     ]
    }
   ],
   "source": [
    "X_de_test, y_de_test = get_padded_sentence_features(sents_de_test, num_features, max_length, wv_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/deu.X.testa.npy', X_de_test)\n",
    "np.save('data/deu.y.testa.npy', y_de_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load German Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_de_train = ConllCorpusReader('data', fileids=['deu.train.txt'], columntypes=('words', 'srl', 'pos', 'ne', 'chunk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_de_train = clean_sents(corpus_de_train.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 10995\n",
      "Processed 100 of 10995\n",
      "Processed 200 of 10995\n",
      "Processed 300 of 10995\n",
      "Processed 400 of 10995\n",
      "Processed 500 of 10995\n",
      "Processed 600 of 10995\n",
      "Processed 700 of 10995\n",
      "Processed 800 of 10995\n",
      "Processed 900 of 10995\n",
      "Processed 1000 of 10995\n",
      "Processed 1100 of 10995\n",
      "Processed 1200 of 10995\n",
      "Processed 1300 of 10995\n",
      "Processed 1400 of 10995\n",
      "Processed 1500 of 10995\n",
      "Processed 1600 of 10995\n",
      "Processed 1700 of 10995\n",
      "Processed 1800 of 10995\n",
      "Processed 1900 of 10995\n",
      "Processed 2000 of 10995\n",
      "Processed 2100 of 10995\n",
      "Processed 2200 of 10995\n",
      "Processed 2300 of 10995\n",
      "Processed 2400 of 10995\n",
      "Processed 2500 of 10995\n",
      "Processed 2600 of 10995\n",
      "Processed 2700 of 10995\n",
      "Processed 2800 of 10995\n",
      "Processed 2900 of 10995\n",
      "Processed 3000 of 10995\n",
      "Processed 3100 of 10995\n",
      "Processed 3200 of 10995\n",
      "Processed 3300 of 10995\n",
      "Processed 3400 of 10995\n",
      "Processed 3500 of 10995\n",
      "Processed 3600 of 10995\n",
      "Processed 3700 of 10995\n",
      "Processed 3800 of 10995\n",
      "Processed 3900 of 10995\n",
      "Processed 4000 of 10995\n",
      "Processed 4100 of 10995\n",
      "Processed 4200 of 10995\n",
      "Processed 4300 of 10995\n",
      "Processed 4400 of 10995\n",
      "Processed 4500 of 10995\n",
      "Processed 4600 of 10995\n",
      "Processed 4700 of 10995\n",
      "Processed 4800 of 10995\n",
      "Processed 4900 of 10995\n",
      "Processed 5000 of 10995\n",
      "Processed 5100 of 10995\n",
      "Processed 5200 of 10995\n",
      "Processed 5300 of 10995\n",
      "Processed 5400 of 10995\n",
      "Processed 5500 of 10995\n",
      "Processed 5600 of 10995\n",
      "Processed 5700 of 10995\n",
      "Processed 5800 of 10995\n",
      "Processed 5900 of 10995\n",
      "Processed 6000 of 10995\n",
      "Processed 6100 of 10995\n",
      "Processed 6200 of 10995\n",
      "Processed 6300 of 10995\n",
      "Processed 6400 of 10995\n",
      "Processed 6500 of 10995\n",
      "Processed 6600 of 10995\n",
      "Processed 6700 of 10995\n",
      "Processed 6800 of 10995\n",
      "Processed 6900 of 10995\n",
      "Processed 7000 of 10995\n",
      "Processed 7100 of 10995\n",
      "Processed 7200 of 10995\n",
      "Processed 7300 of 10995\n",
      "Processed 7400 of 10995\n",
      "Processed 7500 of 10995\n",
      "Processed 7600 of 10995\n",
      "Processed 7700 of 10995\n",
      "Processed 7800 of 10995\n",
      "Processed 7900 of 10995\n",
      "Processed 8000 of 10995\n",
      "Processed 8100 of 10995\n",
      "Processed 8200 of 10995\n",
      "Processed 8300 of 10995\n",
      "Processed 8400 of 10995\n",
      "Processed 8500 of 10995\n",
      "Processed 8600 of 10995\n",
      "Processed 8700 of 10995\n",
      "Processed 8800 of 10995\n",
      "Processed 8900 of 10995\n",
      "Processed 9000 of 10995\n",
      "Processed 9100 of 10995\n",
      "Processed 9200 of 10995\n",
      "Processed 9300 of 10995\n",
      "Processed 9400 of 10995\n",
      "Processed 9500 of 10995\n",
      "Processed 9600 of 10995\n",
      "Processed 9700 of 10995\n",
      "Processed 9800 of 10995\n",
      "Processed 9900 of 10995\n",
      "Processed 10000 of 10995\n",
      "Processed 10100 of 10995\n",
      "Processed 10200 of 10995\n",
      "Processed 10300 of 10995\n",
      "Processed 10400 of 10995\n",
      "Processed 10500 of 10995\n",
      "Processed 10600 of 10995\n",
      "Processed 10700 of 10995\n",
      "Processed 10800 of 10995\n",
      "Processed 10900 of 10995\n"
     ]
    }
   ],
   "source": [
    "X_de_train, y_de_train = get_padded_sentence_features(sents_de_train, num_features, max_length, wv_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/deu.X.train.npy', X_de_train)\n",
    "np.save('data/deu.y.train.npy', y_de_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load English Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_en_test = ConllCorpusReader('data', fileids=['eng.testa.txt'], columntypes=('words', 'pos', 'ne', 'chunk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_en_test = clean_sents(corpus_en_test.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 2701\n",
      "Processed 100 of 2701\n",
      "Processed 200 of 2701\n",
      "Processed 300 of 2701\n",
      "Processed 400 of 2701\n",
      "Processed 500 of 2701\n",
      "Processed 600 of 2701\n",
      "Processed 700 of 2701\n",
      "Processed 800 of 2701\n",
      "Processed 900 of 2701\n",
      "Processed 1000 of 2701\n",
      "Processed 1100 of 2701\n",
      "Processed 1200 of 2701\n",
      "Processed 1300 of 2701\n",
      "Processed 1400 of 2701\n",
      "Processed 1500 of 2701\n",
      "Processed 1600 of 2701\n",
      "Processed 1700 of 2701\n",
      "Processed 1800 of 2701\n",
      "Processed 1900 of 2701\n",
      "Processed 2000 of 2701\n",
      "Processed 2100 of 2701\n",
      "Processed 2200 of 2701\n",
      "Processed 2300 of 2701\n",
      "Processed 2400 of 2701\n",
      "Processed 2500 of 2701\n",
      "Processed 2600 of 2701\n",
      "Processed 2700 of 2701\n"
     ]
    }
   ],
   "source": [
    "X_en_test, y_en_test = get_padded_sentence_features(sents_en_test, num_features, max_length, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/eng.X.testa.npy', X_en_test)\n",
    "np.save('data/eng.y.testa.npy', y_en_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Spanish Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_es = KeyedVectors.load_word2vec_format('data/wiki.multi.es.vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_es_test = ConllCorpusReader('data', fileids=['esp.testa.txt'], columntypes=('words', 'pos', 'chunk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_es_test = clean_sents(corpus_es_test.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 1588\n",
      "Processed 100 of 1588\n",
      "Processed 200 of 1588\n",
      "Processed 300 of 1588\n",
      "Processed 400 of 1588\n",
      "Processed 500 of 1588\n",
      "Processed 600 of 1588\n",
      "Processed 700 of 1588\n",
      "Processed 800 of 1588\n",
      "Processed 900 of 1588\n",
      "Processed 1000 of 1588\n",
      "Processed 1100 of 1588\n",
      "Processed 1200 of 1588\n",
      "Processed 1300 of 1588\n",
      "Processed 1400 of 1588\n",
      "Processed 1500 of 1588\n"
     ]
    }
   ],
   "source": [
    "X_es_test, y_es_test = get_padded_sentence_features(sents_es_test, num_features, max_length, wv_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/esp.X.testa.npy', X_es_test)\n",
    "np.save('data/esp.y.testa.npy', y_es_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Spanish Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_es_train = ConllCorpusReader('data', fileids=['esp.train.txt'], columntypes=('words', 'pos', 'chunk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_es_train = clean_sents(corpus_es_train.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 7036\n",
      "Processed 100 of 7036\n",
      "Processed 200 of 7036\n",
      "Processed 300 of 7036\n",
      "Processed 400 of 7036\n",
      "Processed 500 of 7036\n",
      "Processed 600 of 7036\n",
      "Processed 700 of 7036\n",
      "Processed 800 of 7036\n",
      "Processed 900 of 7036\n",
      "Processed 1000 of 7036\n",
      "Processed 1100 of 7036\n",
      "Processed 1200 of 7036\n",
      "Processed 1300 of 7036\n",
      "Processed 1400 of 7036\n",
      "Processed 1500 of 7036\n",
      "Processed 1600 of 7036\n",
      "Processed 1700 of 7036\n",
      "Processed 1800 of 7036\n",
      "Processed 1900 of 7036\n",
      "Processed 2000 of 7036\n",
      "Processed 2100 of 7036\n",
      "Processed 2200 of 7036\n",
      "Processed 2300 of 7036\n",
      "Processed 2400 of 7036\n",
      "Processed 2500 of 7036\n",
      "Processed 2600 of 7036\n",
      "Processed 2700 of 7036\n",
      "Processed 2800 of 7036\n",
      "Processed 2900 of 7036\n",
      "Processed 3000 of 7036\n",
      "Processed 3100 of 7036\n",
      "Processed 3200 of 7036\n",
      "Processed 3300 of 7036\n",
      "Processed 3400 of 7036\n",
      "Processed 3500 of 7036\n",
      "Processed 3600 of 7036\n",
      "Processed 3700 of 7036\n",
      "Processed 3800 of 7036\n",
      "Processed 3900 of 7036\n",
      "Processed 4000 of 7036\n",
      "Processed 4100 of 7036\n",
      "Processed 4200 of 7036\n",
      "Processed 4300 of 7036\n",
      "Processed 4400 of 7036\n",
      "Processed 4500 of 7036\n",
      "Processed 4600 of 7036\n",
      "Processed 4700 of 7036\n",
      "Processed 4800 of 7036\n",
      "Processed 4900 of 7036\n",
      "Processed 5000 of 7036\n",
      "Processed 5100 of 7036\n",
      "Processed 5200 of 7036\n",
      "Processed 5300 of 7036\n",
      "Processed 5400 of 7036\n",
      "Processed 5500 of 7036\n",
      "Processed 5600 of 7036\n",
      "Processed 5700 of 7036\n",
      "Processed 5800 of 7036\n",
      "Processed 5900 of 7036\n",
      "Processed 6000 of 7036\n",
      "Processed 6100 of 7036\n",
      "Processed 6200 of 7036\n",
      "Processed 6300 of 7036\n",
      "Processed 6400 of 7036\n",
      "Processed 6500 of 7036\n",
      "Processed 6600 of 7036\n",
      "Processed 6700 of 7036\n",
      "Processed 6800 of 7036\n",
      "Processed 6900 of 7036\n",
      "Processed 7000 of 7036\n"
     ]
    }
   ],
   "source": [
    "X_es_train, y_es_train = get_padded_sentence_features(sents_es_train, num_features, max_length, wv_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/esp.X.train.npy', X_es_train)\n",
    "np.save('data/esp.y.train.npy', y_es_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
